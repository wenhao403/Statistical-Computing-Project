# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
DTM <- Ngram_corpus(data$title)
knitr::opts_chunk$set(echo = TRUE)
# Install
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("servr")
# Load
# library(readxl)
# library(tm)
# library(stringi)
# library(stringr)
# library(text2vec)
# library(wordcloud)
# library(servr)
#
# movie_review
#browseVignettes(package = "text2vec")
data = read.csv("bbc_news.csv", encoding = "UTF-8")
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
data$title
?Ngram_corpus
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
DTM <- Ngram_corpus(data$title)
# Install
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("servr")
# Load
# library(readxl)
# library(tm)
# library(stringi)
# library(stringr)
# library(text2vec)
# library(wordcloud)
# library(servr)
#
movie_review
browseVignettes(package = "text2vec")
data = read.csv("bbc_news.csv", encoding = "UTF-8")
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
data
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
# Install
# install.packages("tm")  # for text mining
# install.packages("SnowballC") # for text stemming
# install.packages("wordcloud") # word-cloud generator
# install.packages("RColorBrewer") # color palettes
# install.packages("servr")
# Load
library(readxl)
library(tm)
library(stringi)
library(stringr)
library(text2vec)
library(wordcloud)
library(servr)
#
movie_review
# browseVignettes(package = "text2vec")
data = read.csv("bbc_news.csv", encoding = "UTF-8")
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
data
data[,1]
data[1,]
data[1,1]
data[1,2]
data[1,3]
data[1,4]
data[1,5]
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
data = read.csv("bbc_news.csv", encoding = "UTF-8")
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
data = read.csv("bbc_news.csv", encoding = "latin1")
data
removeNumPunct <- function(x){
gsub("[^[:alpha:][:space:]]*", "", x)
}
Tokenizer <- function(x){
unlist(lapply(ngrams(words(x), 1), paste, collapse = " "),
use.names = FALSE)
}
#toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
Ngram_corpus <- function(corpus, minIgnore=.01, maxIgnore=.8){
corpus <- VCorpus(VectorSource(corpus))
# transform words to lower cases
corpus <- tm_map(corpus, content_transformer(stri_trans_tolower))
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# remove punctuations
corpus <- tm_map(corpus, removePunctuation)
# remove stopwords
corpus <- tm_map(corpus, removeWords, stopwords("en"))
# corpus <- tm_map(corpus, removeWords, c("say", "says"))
# Stemming
corpus <- tm_map(corpus, stemDocument)
corpus_len <- length(corpus)
minDocFreq <- corpus_len * minIgnore
maxDocFreq <- corpus_len * maxIgnore
dtm <- DocumentTermMatrix(corpus,
control=list(tokenize=Tokenizer,
removePunctuation=TRUE,
global=c(minDocFreq, maxDocFreq),
weighting=function(x)
weightTf(x)))
return(dtm)
}
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
library(readxl)
library(tm)
library(stringi)
library(stringr)
library(text2vec)
library(wordcloud)
library(servr)
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
# Install
# install.packages("tm")  # for text mining
install.packages("SnowballC") # for text stemming
ntopics <- 5
n.iter <- 1000
conv.tol <- 0.0001
n.conv <- 25
perp = c()
DTM <- Ngram_corpus(data$title)
# The input for the LDA model must be CsparseMatrix
lda.dtM.title <- as(as.matrix(DTM), 'CsparseMatrix')
a = rep(0,100)
#inspect(DTM)
perp_ntopic = function(i, x, n.iter, conv.tol, n.conv, alpha=0.1, beta=0.001){
lda_model = LDA$new(n_topics=i, doc_topic_prior = alpha, topic_word_prior = beta)
doc_topic_distr =   lda_model$fit_transform(x=x,
n_iter=n.iter,
convergence_tol=conv.tol,
n_check_convergence = n.conv,
progressbar = FALSE)
topic_word_distr = lda_model$topic_word_distribution
perp[i] = perplexity(x, topic_word_distr, doc_topic_distr)
}
perp = sapply(1:100, function(i) perp_ntopic(i, lda.dtM.title, n.iter, conv.tol, n.conv))
plot(seq(1,100),perp[1:100],main = "Perplexity plot with news titles",xlab = "number of topics",ylab = "perplexity(K)")
abline(v = 30,col="red")
text(31,10, "ntopics = 30 ", col = "blue", adj = c(0, -20))
ntopics <- 20
# initialize LDA
lda_model_title = LDA$new(n_topics = ntopics, doc_topic_prior = 0.1, topic_word_prior = 0.001)
# fit the data, the output provides the topic distribution
doc_topic_distr =   lda_model_title$fit_transform(x=lda.dtM.title,
n_iter=n.iter,
convergence_tol=conv.tol,
n_check_convergence = 25,
progressbar = FALSE)
# retrieve the word distribution over each topic
topic_word_distr = lda_model_title$topic_word_distribution
#lda_model_title$get_top_words(n=50)
for (i in 1:ntopics) {
set.seed(1)
wordcloud(words = lda_model_title$get_top_words(n = 50)[,i], freq = sort(topic_word_distr[i,],decreasing = T)[1:50], random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
}
i=1
wordcloud(words = lda_model_title$get_top_words(n = 50)[i,], freq = sort(topic_word_distr[i,],decreasing = T)[1:50], random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
# plot the Intertopic Distance Map for each topic
#lda_model$plot()
# calculate perplexity
perp_title_30 <- perplexity(lda.dtM.title, topic_word_distr, doc_topic_distr)
perp = c()
DTM <- Ngram_corpus(data$description)
# The input for the LDA model must be CsparseMatrix
lda.dtM.dscrp <- as(as.matrix(DTM), 'CsparseMatrix')
a = rep(0,100)
#inspect(DTM)
perp = sapply(1:100, function(i) perp_ntopic(i, lda.dtM.dscrp, n.iter, conv.tol, n.conv,
alpha = 0.1, beta = 0.01))
plot(seq(1,100),perp[1:100],main = "Perplexity plot with news titles",xlab = "number of topics",ylab = "perplexity(K)")
abline(v = 30,col="red")
text(31,10, "ntopics = 40 ", col = "blue", adj = c(0, -20))
ntopics <- 30
# initialize LDA
lda_model_dscrp = LDA$new(n_topics = ntopics)
# fit the data, the output provides the topic distribution
doc_topic_distr =   lda_model_dscrp$fit_transform(x=lda.dtM.dscrp,
n_iter=n.iter,
convergence_tol=conv.tol,
n_check_convergence = 25,
progressbar = FALSE)
# retrieve the word distribution over each topic
topic_word_distr = lda_model_dscrp$topic_word_distribution
#lda_model_dscrp$get_top_words(n=5)
# plot the wordcloud for each topic
for (i in 1:ntopics) {
set.seed(1)
wordcloud(words = lda_model_dscrp$get_top_words(n = 50)[,i], freq = sort(topic_word_distr[i,],decreasing = T)[1:50], random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
}
# plot the Intertopic Distance Map for each topic
#lda_model$plot()
# calculate perplexity
perp_dscrp_30 <- perplexity(lda.dtM.dscrp, topic_word_distr, doc_topic_distr)
View(DTM)
View(data)
shiny::runApp('home/nthu1102/Statistical_computing/final_demo/SC_final')
runApp('home/nthu1102/Statistical_computing/final_demo/SC_final')
runApp('home/nthu1102/Statistical_computing/final_demo/SC_final')
runApp('home/nthu1102/Statistical_computing/final_demo/SC_final')
shiny::runApp('home/nthu1102/Statistical_computing/final_demo/SC_final')
